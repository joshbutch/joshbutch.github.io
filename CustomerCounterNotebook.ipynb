{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Practicum I - Real-Time Object Detection/Tracking for Retail Business Intelligence</h2>\n",
    "<h4>Regis University - CC&IS Department - Data Science</h4>\n",
    "<h5>Practicum Advisor: Prof. Aiman Gannous</h5>\n",
    "<h5>Student: Josh Butch</h5>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A way to use artificial neural networks and video cameras to learn more about the habits of retail customers.  The purpose of this project is to identify opportunities to improve the retail customer shopping experience by gathering unique intelligence about their shopping habits/experiences.<br>\n",
    "\n",
    "The following customer counter allows for the tracking and detection of human objects utilizing a standard personal computer and a high definition webcam.  This model can be configured to work with the current multi-camera setup that the store uses for security.  The ability to track centroids across multiple cameras will greatly enhance the usefulness of this model in gathering shopping intel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First things first - we are going to import the necessary packages for this model.  It's not an extensive amount of code, but there are multiple packages necessary to accomplish our goals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary packages\n",
    "from pyimagesearch.centroidtracker import CentroidTracker  # Centroid tracking capability\n",
    "from pyimagesearch.trackableobject import TrackableObject  # Creates a trackable object ID\n",
    "from imutils.video import VideoStream                      # Package to capture external video stream\n",
    "from imutils.video import FPS                              # Package to track and count frames per second\n",
    "import numpy as np                                         # Abbreviate numpy as np\n",
    "import argparse                                            # Package to write and parse arguments\n",
    "import imutils                                             # Basic image processing package\n",
    "import time                                                # Package to track time\n",
    "import dlib                                                # Package to make real world machine learning apps\n",
    "import cv2                                                 # OpenCV package for deep neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the packages are installed it's time to create and parse the arguments for this model.  The following arguments will allow the user of the model to modify model inputs, outputs, video sources, confidence intervals, and frame detection rates.  Without these arguments that model would not be able to take inputs from the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct and parse the arguments\n",
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument(\"-p\", \"--prototxt\", required=True,\n",
    "\thelp=\"path to Caffe 'deploy' prototxt file\")\n",
    "ap.add_argument(\"-m\", \"--model\", required=True,\n",
    "\thelp=\"path to Caffe pre-trained model\")\n",
    "ap.add_argument(\"-i\", \"--input\", type=str,\n",
    "\thelp=\"path to optional input video file\")\n",
    "ap.add_argument(\"-o\", \"--output\", type=str,\n",
    "\thelp=\"path to optional output video file\")\n",
    "ap.add_argument(\"-c\", \"--confidence\", type=float, default=0.4,\n",
    "\thelp=\"minimum probability to filter weak detections\")\n",
    "ap.add_argument(\"-s\", \"--skip-frames\", type=int, default=30,\n",
    "\thelp=\"# of skip frames between detections\")\n",
    "args = vars(ap.parse_args())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll be initializing the list of class labels that our model is trained to detect.  In this model we are using the CaffeModel training dataset.  These list of classes are the subcategories that our frames will be referenced against.  In the end the only class we'll be detecting is the \"person\" class.\n",
    "\n",
    "We'll also take the opportunity to call up the model arguments by assigning \"prototxt\" and \"model\" args at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the list of class labels MobileNet SSD was trained to\n",
    "# detect\n",
    "CLASSES = [\"background\", \"aeroplane\", \"bicycle\", \"bird\", \"boat\",\n",
    "\t\"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\",\n",
    "\t\"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\", \"sheep\",\n",
    "\t\"sofa\", \"train\", \"tvmonitor\"]\n",
    "\n",
    "# Load the serialized model from disk\n",
    "print(\"[INFO] loading model...\")\n",
    "net = cv2.dnn.readNetFromCaffe(args[\"prototxt\"], args[\"model\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll determine whether or not the video input will be from a webcam or a video file.  In this instance we'll be connecting using our webcam.  This is indicating by the \"src=0\" path in the below code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If a video path was not supplied, reference the webcam\n",
    "if not args.get(\"input\", False):\n",
    "\tprint(\"[INFO] starting video stream...\")\n",
    "\tvs = VideoStream(src=0).start()          # src=0 is the webcam\n",
    "\ttime.sleep(2.0)                          # Allows a \"warm-up\" period if there's a delay in webcam activation\n",
    "\n",
    "# Otherwise, grab a reference to the video file being inputted\n",
    "else:\n",
    "\tprint(\"[INFO] opening video file...\")\n",
    "\tvs = cv2.VideoCapture(args[\"input\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following piece of code is to initialize a video writer and frame dimensions.  We will be addressing both of these instances a little further in the code by allowing the frame size to automatically set dimensions.  We also don't need a video writer in this instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the video writer if needed\n",
    "writer = None\n",
    "\n",
    "# Initialize the frame dimensions or set them from the first frame of video\n",
    "W = None\n",
    "H = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code are the last pieces to be instantiated before we begin looping over the frames to begin our object detection/tracking.  The centroid tracker is the key variable that identifies an object being tracked.  The centroid IS the object as far as the model is concerned.  Intializing the FPS counters and estimator are complete as well so now we can begin to analyze each frame to detect our desired objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate our centroid tracker \n",
    "ct = CentroidTracker(maxDisappeared=40, maxDistance=50)\n",
    "trackers = []           # Initialize a list to store each of our dlib correlation trackers\n",
    "trackableObjects = {}   # Intialize a dictionary to map each unique object ID to a TrackableObject\n",
    "\n",
    "# Initialize the total number of frames processed  \n",
    "totalFrames = 0\n",
    "totalDown = 0    # Total # of objects down\n",
    "totalUp = 0      # Total # of objects up\n",
    "\n",
    "# Start the frames per second throughput estimator\n",
    "fps = FPS().start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beginning our for loop to begin to iterate over the frames and detect/track objects.  This is where the heart of the model begins to run it's object detection analysis.  The first part of the loop determines the video source and how to determine if the video is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over frames from vs\n",
    "while True:\n",
    "\t# Capture the next frame and determine either VideoCapture or VideoStream\n",
    "\tframe = vs.read()\n",
    "\tframe = frame[1] if args.get(\"input\", False) else frame\n",
    "\n",
    "\t# If we are viewing a video and there's no frame to grab we've reached the end\n",
    "\tif args[\"input\"] is not None and frame is None:\n",
    "\t\tbreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we resize the frame and accomplish color inversion to help in edge detection.  As stated in the comments, if the frame dimension are empty the frame size will be set according to it's shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\t# Resize the frame to have a maximum width of 500 pixels \n",
    "\tframe = imutils.resize(frame, width=500)\n",
    "\trgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert the frame from BGR to RGB for dlib\n",
    "\n",
    "\t# If the frame dimensions are empty, set them according to shape\n",
    "\tif W is None or H is None:\n",
    "\t\t(H, W) = frame.shape[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll initialize the video writer if one is necessary.  We're not creating an output video file, but it's a really nice feature to be able to create a file of your model working over a particular piece of video that's interesting to analyze.  One thing that became apparent is the amount of nonproductive detection time that a model like this exhibits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\t# Initialize the writer if necessary\n",
    "\tif args[\"output\"] is not None and writer is None:\n",
    "\t\tfourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n",
    "\t\twriter = cv2.VideoWriter(args[\"output\"], fourcc, 30,\n",
    "\t\t\t(W, H), True)\n",
    "\n",
    "\t# Initialize the current status along with our list of bounding\n",
    "\t# box rectangles returned by either (1) our object detector or\n",
    "\t# (2) the correlation trackers\n",
    "\tstatus = \"Waiting\"\n",
    "\trects = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next section is where the magic happens as we convert our frame to a blob in a single pass.  After converting the frame to a blob the model begins to loop over detections in an attempt to annotate their location and classify the object being detected as the frames progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\t\t# Convert the frame to a blob and pass the blob through the\n",
    "\t\t# network and obtain the object detections\n",
    "\t\tblob = cv2.dnn.blobFromImage(frame, 0.007843, (W, H), 127.5)\n",
    "\t\tnet.setInput(blob)\n",
    "\t\tdetections = net.forward()\n",
    "\n",
    "\t\t# loop over the detections\n",
    "\t\tfor i in np.arange(0, detections.shape[2]):\n",
    "\t\t\tconfidence = detections[0, 0, i, 2]     # Extract the probability associated with the detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following section we filter out weak detections based on our confidence rating.  Also, now that we have classified our detected object, we can drop any classes that we aren't interested in detecting.  In this case the only class we want to identify is \"person.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\t\t\t# Filter out weak detections \n",
    "\t\t\tif confidence > args[\"confidence\"]:\n",
    "\t\t\t\tidx = int(detections[0, 0, i, 1])  # Extract the index of the class label from the detections list\n",
    "\n",
    "\t\t\t\t# If the class label is not a person, ignore it\n",
    "\t\t\t\tif CLASSES[idx] != \"person\":\n",
    "\t\t\t\t\tcontinue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next section of code will allow the model to compute the coordinates of each object's bounding box.  Once constructed the list of trackers will be appended to add any new objects being tracked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\t\t\t\t# Compute the coordinates of the bounding box for the object\n",
    "\t\t\t\tbox = detections[0, 0, i, 3:7] * np.array([W, H, W, H])\n",
    "\t\t\t\t(startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\n",
    "\t\t\t\t from the bounding\n",
    "\t\t\t\ttracker = dlib.correlation_tracker()  # Construct a dlib rectangle object\n",
    "\t\t\t\trect = dlib.rectangle(startX, startY, endX, endY)\n",
    "\t\t\t\ttracker.start_track(rgb, rect)        # Start the dlib correlation tracker\n",
    "\n",
    "\t\t\t\ttrackers.append(tracker)  # Add to list of trackers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following else statement determines the best option for processing throughput.  It will loop over the trackers, setting the status as it does, which it then updates with a new position.  Once the position has the start and ending X,Y coordinates the model can add the bounding box to the object being detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\t# Utilize our object *trackers* rather than object *detectors* to obtain a higher frame processing throughput\n",
    "\telse:\n",
    "\t\t# Loop over the trackers\n",
    "\t\tfor tracker in trackers:\n",
    "\t\t\tstatus = \"Tracking\"  # Set the status of our system to be 'tracking'\n",
    "\n",
    "\t\t\t# Update the tracker and grab the updated position\n",
    "\t\t\ttracker.update(rgb)\n",
    "\t\t\tpos = tracker.get_position()\n",
    "\n",
    "\t\t\t# Unpack the position object\n",
    "\t\t\tstartX = int(pos.left())\n",
    "\t\t\tstartY = int(pos.top())\n",
    "\t\t\tendX = int(pos.right())\n",
    "\t\t\tendY = int(pos.bottom())\n",
    "\n",
    "\t\t\t# Add the bounding box coordinates to the rectangles list\n",
    "\t\t\trects.append((startX, startY, endX, endY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The horizontal reference line is the key piece of code when determining an objects movement.  In order to track an object there must be a spatial reference or line of demarcation that the centroid has to cross in one direction or another.  We are trying to determine entry and exit movements so we have a single plane tracking two directions of movement for our purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\t# Draw a horizontal line in the center of the frame -- once an\n",
    "\t# object crosses this line we will determine whether they were\n",
    "\t# moving 'up' or 'down'\n",
    "\tcv2.line(frame, (0, H // 2), (W, H // 2), (0, 255, 255), 2)\n",
    "\n",
    "\tobjects = ct.update(rects)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop over the tracked objects to determine if one exists for that object ID.  If a trackable object does not exist for that object ID the model will create one, otherwise it will update the information of the previous centroid and, based on a positive or negative number, will be able to determine direction of movement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\t# Loop over the tracked objects\n",
    "\tfor (objectID, centroid) in objects.items():\n",
    "\t\tto = trackableObjects.get(objectID, None) # Check to see if a trackable object exists for current object ID\n",
    "\n",
    "\t\t# If none, create one\n",
    "\t\tif to is None:\n",
    "\t\t\tto = TrackableObject(objectID, centroid)\n",
    "\n",
    "\t\telse:\n",
    "\t\t\t# The difference between the y-coordinate of the *current*\n",
    "\t\t\t# centroid and the mean of *previous* centroids will tell\n",
    "\t\t\t# us in which direction the object is moving (negative for\n",
    "\t\t\t# 'up' and positive for 'down')\n",
    "\t\t\ty = [c[1] for c in to.centroids]\n",
    "\t\t\tdirection = centroid[1] - np.mean(y)\n",
    "\t\t\tto.centroids.append(centroid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we're going to analyze the direction of travel and tally whether or not that object has been counted.  The positive or negative resulting number will again determine the direction of travel and how the model classifies that object's movement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\t\t\t# Check to see if the object has been counted or not\n",
    "\t\t\tif not to.counted:\n",
    "\t\t\t\t# If the direction is negative (indicating the object\n",
    "\t\t\t\t# is moving up) AND the centroid is above the center\n",
    "\t\t\t\t# line, count the object\n",
    "\t\t\t\tif direction < 0 and centroid[1] < H // 2:\n",
    "\t\t\t\t\ttotalUp += 1\n",
    "\t\t\t\t\tto.counted = True\n",
    "\n",
    "\t\t\t\t# If the direction is positive (indicating the object\n",
    "\t\t\t\t# is moving down) AND the centroid is below the\n",
    "\t\t\t\t# center line, count the object\n",
    "\t\t\t\telif direction > 0 and centroid[1] > H // 2:\n",
    "\t\t\t\t\ttotalDown += 1\n",
    "\t\t\t\t\tto.counted = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section we are going to store the trackable object we created with the objectID.  Once that's accomplished we'll be able to print the centroid and object ID on the output frame for labeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\t\t# Store the trackable object\n",
    "\t\ttrackableObjects[objectID] = to\n",
    "\n",
    "\t\t# Draw both the ID of the object and the centroid of the\n",
    "\t\t# object on the output frame\n",
    "\t\ttext = \"ID {}\".format(objectID)\n",
    "\t\tcv2.putText(frame, text, (centroid[0] - 10, centroid[1] - 10),\n",
    "\t\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\t\tcv2.circle(frame, (centroid[0], centroid[1]), 4, (0, 255, 0), -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is what I refer to as \"cleanup code\" that essentially releases/closes any connections/dependencies that may need to be closed before exiting the program.  Disk writing is accomplished if necessary, outputs are validated, key breaks instituted, and counting increments/decrements adjusted based on direction of centroid travel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-1-d6a0ce9e9f23>, line 25)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-d6a0ce9e9f23>\"\u001b[1;36m, line \u001b[1;32m25\u001b[0m\n\u001b[1;33m    writer.release()\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "\t# Check to see if we should write the frame to disk\n",
    "\tif writer is not None:\n",
    "\t\twriter.write(frame)\n",
    "\n",
    "\t# Show the output frame\n",
    "\tcv2.imshow(\"Frame\", frame)\n",
    "\tkey = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "\t# If the `q` key was pressed, break from the loop\n",
    "\tif key == ord(\"q\"):\n",
    "\t\tbreak\n",
    "\n",
    "\t# Increment the total number of frames processed thus far and\n",
    "\t# then update the FPS counter\n",
    "\ttotalFrames += 1\n",
    "\tfps.update()\n",
    "\n",
    "# Stop the timer and display FPS information\n",
    "fps.stop()\n",
    "print(\"[INFO] elapsed time: {:.2f}\".format(fps.elapsed()))\n",
    "print(\"[INFO] approx. FPS: {:.2f}\".format(fps.fps()))\n",
    "\n",
    "# Check to see if we need to release the video writer pointer\n",
    "if writer is not None:\n",
    "\twriter.release()\n",
    "\n",
    "# If not using a video file, stop the camera video stream\n",
    "if not args.get(\"input\", False):\n",
    "\tvs.stop()\n",
    "\n",
    "# Release the video file pointer\n",
    "else:\n",
    "\tvs.release()\n",
    "\n",
    "# Close any open windows\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, we've managed to create a working customer counter that will monitor a single point of entry/exit.  Although this is the most basic of customer counters, it's a solid foundation to start from when adding additional cameras and path tracking.  Further modification of this file will allow for operational intelligence and customer behavior insights that weren't previously available."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
